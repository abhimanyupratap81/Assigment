{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What is a parameter?\n",
        "\n",
        "--> A parameter typically refers to a value that influences how features are created, transformed, or selectedâ€”but it is not learned from the data automatically like model parameters (e.g., weights in a neural network)."
      ],
      "metadata": {
        "id": "OqYf2lhQ95DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is correlation?\n",
        "# What does negative correlation mean?\n",
        "\n",
        "--> Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "1. It is usually expressed as a number between -1 and 1.\n",
        "\n",
        "2. A common measure is the Pearson correlation coefficient (r).\n",
        "\n",
        "Negative Correlation:\n",
        "As one variable increases, the other decreases.\n",
        "\n",
        "Example: Speed and travel time â€” the faster you go, the less time it takes."
      ],
      "metadata": {
        "id": "1muvHbLm-VMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "--> Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables systems to learn patterns from data and make decisions or predictions without being explicitly programmed for specific tasks. The main components in machine learning are:\n",
        "\n",
        "1. Data: Raw input used for learning.\n",
        "\n",
        "2. Features: Variables extracted from data (input).\n",
        "\n",
        "3. Labels/Targets: Desired output values (used in supervised learning).\n",
        "\n",
        "4. Model: Mathematical structure that learns from data.\n",
        "\n",
        "5. Algorithm: The method for training the model (e.g., gradient descent).\n",
        "\n",
        "6. Training: Process of feeding data to the model to learn.\n",
        "\n",
        "7. Testing/Validation: Process of evaluating model performance on unseen data.\n",
        "\n",
        "8. Loss Function / Cost Function: Measures error between prediction and actual output.\n",
        "\n",
        "9. Optimization: Technique to minimize the loss function (e.g., using gradient descent).\n",
        "\n",
        "10. Hyperparameters: Settings/configurations chosen before training (e.g., learning rate, batch size).\n",
        "\n",
        "11. Parameters: Values learned by the model during training (e.g., weights in neural networks).\n",
        "\n",
        "12. Evaluation Metrics: Metrics used to measure model performance (e.g., accuracy, RMSE).\n",
        "\n",
        "13. Prediction / Inference: Using the trained model to make predictions on new data.\n",
        "\n",
        "14. Overfitting / Underfitting: Concepts related to model generalization.\n",
        "\n",
        "15. Cross-Validation: Technique to evaluate model stability and performance.\n",
        "\n",
        "16. Feature Engineering: Creating, selecting, and transforming input features.\n",
        "\n",
        "17. Data Preprocessing: Cleaning, normalizing, or transforming raw data.\n",
        "\n",
        "18. Model Deployment: Integrating the trained model into a real-world system.\n",
        "\n",
        "19. Monitoring & Maintenance: Tracking model performance over time and updating as needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "OkibnuTN-u7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "--> The loss value in machine learning is a key indicator of how well a model is performing. It measures the difference between the model's predictions and the actual target values. A lower loss value generally means that the modelâ€™s predictions are closer to the true outputs, indicating better performance. During training, the loss helps guide the learning processâ€”if the loss decreases over time, it suggests the model is learning meaningful patterns from the data. Moreover, comparing loss values on training and validation sets can reveal important insights: for example, if the training loss continues to decrease but the validation loss starts increasing, this may indicate overfitting, where the model memorizes the training data but fails to generalize to unseen data. Loss functions also help in optimizing the model by informing algorithms like gradient descent how to adjust the modelâ€™s internal parameters. Although the loss is a powerful tool for evaluating model quality, it is typically used alongside other metrics like accuracy or F1-score, especially in classification problems, to get a more complete picture of performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "19HG1Zw7ARuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are continuous and categorical variables?\n",
        "\n",
        "--> In data analysis and machine learning, variables (also called features or attributes) are the columns of your dataset. They are usually classified into two main types: continuous and categorical.\n",
        "\n",
        "1.  Continuous Variables: Continuous variables are numeric values that can take any value within a range. They are often measurable and can have decimal points.\n",
        "\n",
        "2. Categorical Variables: Categorical variables represent distinct groups or categories. They usually take on a limited number of values and describe qualities or labels, not quantities."
      ],
      "metadata": {
        "id": "QvNEOAP9Ak1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "--> Categorical variables cannot be directly fed into most machine learning models because they are non-numeric. So, we must convert them into numerical form using encoding techniques.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "1. Label Encoding\n",
        "Converts each category into a unique integer.\n",
        "\n",
        "Useful for ordinal variables (where order matters).\n",
        "\n",
        "Example:\n",
        "[\"Low\", \"Medium\", \"High\"] â†’ [0, 1, 2]\n",
        "\n",
        "Pros: Simple\n",
        "Cons: Implies an order even when one doesnâ€™t exist (bad for nominal variables)\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Useful for nominal variables (no inherent order).\n",
        "\n",
        "Example:\n",
        "[\"Red\", \"Blue\", \"Green\"] â†’ [[1,0,0], [0,1,0], [0,0,1]]\n",
        "\n",
        "Pros: No false ordering\n",
        "Cons: Can create many columns (sparse) if there are lots of categories\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Like label encoding, but used intentionally when order matters.\n",
        "\n",
        "Example:\n",
        "[\"Poor\", \"Average\", \"Good\"] â†’ [1, 2, 3]\n",
        "\n",
        "4. Binary Encoding\n",
        "Combines label and one-hot encoding.\n",
        "\n",
        "Converts categories to binary code, then splits into columns.\n",
        "\n",
        "Example:\n",
        "Category 3 â†’ 011 â†’ [0, 1, 1]\n",
        "\n",
        "Pros: Reduces dimensionality\n",
        "Cons: More complex to interpret\n",
        "\n",
        "5. Target Encoding (Mean Encoding)\n",
        "Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "Example:\n",
        "If customers from \"City A\" have an average purchase amount of $50, encode \"City A\" as 50.\n",
        "\n",
        "Pros: Can boost performance\n",
        "Cons: Prone to overfitting on small datasets (use with cross-validation or smoothing)\n",
        "\n",
        "6. Frequency / Count Encoding\n",
        "Replaces categories with the frequency or count of how often each appears.\n",
        "\n",
        "Example:\n",
        "If \"Red\" appears 100 times, encode it as 100."
      ],
      "metadata": {
        "id": "7fnZQfkOA5pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What do you mean by training and testing a dataset?\n",
        "\n",
        "--> In machine learning, training and testing a dataset refers to splitting your data into two parts so that your model can learn from one part (training) and be evaluated on the other part (testing).\n",
        "\n",
        "1. Training Dataset\n",
        "The training dataset is the portion of the data used to teach the model.\n",
        "\n",
        "The model uses this data to learn patterns by adjusting internal parameters (like weights in a neural network).\n",
        "\n",
        "It's like studying for an examâ€”this is the data the model \"sees\" and learns from.\n",
        "\n",
        "2. Testing Dataset\n",
        "The testing dataset is used to evaluate the modelâ€™s performance after training.\n",
        "\n",
        "It contains new data that the model hasnâ€™t seen before.\n",
        "\n",
        "This helps you understand how well the model can generalize to unseen data.\n",
        "\n",
        "Itâ€™s like the final examâ€”you test what the model learned without giving it the answers."
      ],
      "metadata": {
        "id": "SkKqkbm0DvAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is sklearn.preprocessing?\n",
        "\n",
        "--> sklearn.preprocessing is a module in Scikit-learn (a popular Python machine learning library) that provides tools to prepare and transform data before feeding it into a machine learning model.\n"
      ],
      "metadata": {
        "id": "Inux8ZmWEOCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is a Test set?\n",
        "\n",
        "--> A test set is a portion of your dataset that is kept separate from the data used to train your machine learning model. Its main purpose is to evaluate the performance of the trained model on new, unseen data. By using the test set, you can estimate how well your model will perform in real-world scenarios on data it hasnâ€™t encountered before. This helps to check if the model generalizes well or if it is overfitting (performing well only on the training data but poorly on new data). Typically, the test set is around 20-30% of the original dataset and is only used after the model has been fully trained and tuned.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zjbansj3ExMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do we split data for model fitting (training and testing) in Python?\n",
        "# How do you approach a Machine Learning problem?\n",
        "\n",
        "--> Start with your entire dataset containing all examples and their corresponding labels (if supervised learning).\n",
        "\n",
        "Decide on the proportion of data to be used for training and testing â€” for example, 80% training and 20% testing.\n",
        "\n",
        "Randomly shuffle the dataset to ensure data points are mixed well and not ordered by any specific feature or time.\n",
        "\n",
        "Split the dataset into two parts:\n",
        "\n",
        "The training set (e.g., 80%) will be used to teach the model â€” it will learn patterns and relationships from this data.\n",
        "\n",
        "The testing set (e.g., 20%) will be kept aside and used only after training to evaluate how well the model generalizes to new data.\n",
        "\n",
        "Use the training data to fit the model, and only after training, use the test set to evaluate its performance.\n",
        "\n",
        "How to Approach a Machine Learning Problem (Algorithm)\n",
        "Understand the problem\n",
        "Identify the task type (classification, regression, clustering), and what success looks like (metrics, goals).\n",
        "\n",
        "Gather and inspect data\n",
        "Collect data and perform exploratory analysis to understand distributions, missing values, and relationships.\n",
        "\n",
        "Prepare the data\n",
        "Clean it by handling missing values and outliers, engineer features that may help the model, encode categorical variables, and scale or normalize numerical data if necessary.\n",
        "\n",
        "Split the data\n",
        "Divide it into training and testing sets (and optionally validation sets) to properly assess model performance.\n",
        "\n",
        "Select a model\n",
        "Choose an initial algorithm suitable for the problem, starting with simple models before moving to more complex ones.\n",
        "\n",
        "Train the model\n",
        "Use the training data to fit the model, enabling it to learn patterns.\n",
        "\n",
        "Validate and tune\n",
        "Use a validation set or cross-validation to fine-tune hyperparameters and avoid overfitting.\n",
        "\n",
        "Evaluate the model\n",
        "Test the model on unseen data (test set) and calculate relevant performance metrics.\n",
        "\n",
        "Deploy and monitor\n",
        "If satisfactory, deploy the model into production and monitor its ongoing performance to update or retrain as needed."
      ],
      "metadata": {
        "id": "x-dn9iFFFLm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "--> We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the data deeply and identify potential issues that could affect model performance. Hereâ€™s why EDA is important:\n",
        "\n",
        "Identify Data Quality Issues: EDA reveals missing values, duplicates, or incorrect data entries that need cleaning before training.\n",
        "\n",
        "Understand Distributions and Patterns: By visualizing and summarizing data, you can see how features are distributed, detect outliers, and understand relationships between variables, which helps in feature selection and engineering.\n",
        "\n",
        "Detect Relationships and Correlations: Knowing which features are strongly correlated with the target (or with each other) informs feature selection and helps avoid redundant or irrelevant variables.\n",
        "\n",
        "Choose the Right Model and Techniques: The nature of the data (e.g., linear vs. nonlinear relationships, categorical vs. continuous variables) guides the choice of models and preprocessing steps like encoding or scaling.\n",
        "\n",
        "Prevent Garbage In, Garbage Out: If the data is flawed or misunderstood, the model will learn incorrect patterns, leading to poor predictions. EDA helps ensure the data is suitable for modeling.\n",
        "\n",
        "Hypothesis Generation: EDA can suggest hypotheses or insights that may improve modeling strategies or business decisions."
      ],
      "metadata": {
        "id": "5IK7MEvhINfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is correlation?\n",
        "\n",
        "--> Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It tells you how much one variable tends to change when the other variable changes.\n",
        "\n",
        "If two variables move together in the same direction, they have a positive correlation (when one increases, the other also increases).\n",
        "\n",
        "If one variable moves in the opposite direction to the other, they have a negative correlation (when one increases, the other decreases).\n",
        "\n",
        "If there is no consistent pattern in how the variables move together, they are said to have no correlation or zero correlation."
      ],
      "metadata": {
        "id": "Fp4PiiHcKmj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does negative correlation mean?\n",
        "\n",
        "--> Negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, the two variables move in opposite directions.\n",
        "\n",
        "A negative correlation value lies between -1 and 0:\n",
        "\n",
        "A value of -1 indicates a perfect negative correlation, meaning the relationship is exactly opposite.\n",
        "\n",
        "A value close to 0 means a weak or no correlation."
      ],
      "metadata": {
        "id": "sSAah2_4LW7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How can you find correlation between variables in Python?\n",
        "\n",
        "--> To find the correlation between variables in Python, especially in a dataset (like a DataFrame), you typically use the Pandas library. Hereâ€™s the algorithmic explanation (no code):\n",
        "\n",
        "ðŸ” Steps to Find Correlation Between Variables in Python:\n",
        "Load your dataset\n",
        "You begin with your data in a tabular format (like a CSV), loaded into a Pandas DataFrame.\n",
        "\n",
        "Choose the columns you want to compare\n",
        "You can calculate correlation between two specific variables or among all numeric variables in the dataset.\n",
        "\n",
        "Use the correlation function\n",
        "Pandas provides a built-in method that calculates correlation coefficients (typically Pearson by default). This method produces either:\n",
        "\n",
        "A single value if comparing two variables.\n",
        "\n",
        "A correlation matrix if comparing multiple variables.\n",
        "\n",
        "Interpret the result\n",
        "The output is a value (or matrix of values) ranging from -1 to +1:\n",
        "\n",
        "+1 â†’ Perfect positive correlation\n",
        "\n",
        "-1 â†’ Perfect negative correlation\n",
        "\n",
        "0 â†’ No correlation\n",
        "\n",
        "(Optional) Visualize it\n",
        "You can use visualization tools like heatmaps (e.g., from Seaborn) to get a graphical representation of correlations, making it easier to spot strong or weak relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "gv4nwx-bLzov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "--> Causation refers to a cause-and-effect relationship, where one variable directly influences or brings about a change in another. In other words, if variable X causes variable Y, then changing X will result in a change in Y. This is different from correlation, which simply measures the statistical association between two variablesâ€”how they move togetherâ€”but does not imply that one causes the other. For example, there may be a correlation between ice cream sales and drowning incidents, as both tend to increase during the summer. However, eating ice cream does not cause drowning; a third factor (hot weather) is influencing both. This is correlation without causation. On the other hand, pressing the accelerator pedal in a car causes it to speed upâ€”this is a clear example of causation, where one action directly affects the outcome. Understanding the difference is critical in data analysis and machine learning, because while correlation can help identify patterns or relationships, only causation can support reliable decision-making and explain why those patterns exist."
      ],
      "metadata": {
        "id": "jlxHt4PSM3WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "--> An optimizer in machine learning (especially in neural networks) is an algorithm used to adjust the weights and biases of a model in order to minimize the loss function. It essentially guides the model in learning the best parameters by reducing the difference between the predicted output and the actual output (i.e., the error or loss). Optimizers are a key part of the training process and determine how efficiently and effectively a model learns.\n",
        "\n",
        "Here are some widely used optimizers, along with explanations and examples:\n",
        "\n",
        "1. Gradient Descent\n",
        "How it works: Calculates the gradient (slope) of the loss function and updates weights in the direction that reduces the loss.\n",
        "\n",
        "Limitation: Can be slow and may get stuck in local minima.\n",
        "\n",
        "Example: Updating weights in a linear regression model using a fixed learning rate.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "How it works: Unlike regular gradient descent, which uses the entire dataset, SGD updates weights using only one data point (or a small batch) at a time.\n",
        "\n",
        "Advantage: Faster and more memory efficient.\n",
        "\n",
        "Disadvantage: Noisy updates can lead to less stable convergence.\n",
        "\n",
        "Example: Training a deep learning model using mini-batches of 32 images at a time.\n",
        "\n",
        "3. Momentum\n",
        "How it works: Improves SGD by adding a \"momentum\" term that considers past gradients, helping it move faster and avoid getting stuck.\n",
        "\n",
        "Advantage: Helps smooth out updates and accelerate convergence.\n",
        "\n",
        "Example: A ball rolling down a hill gains momentum and doesn't get stuck in small dips.\n",
        "\n",
        "4. AdaGrad\n",
        "How it works: Adapts the learning rate for each parameter individually based on the frequency of updatesâ€”smaller updates for frequently updated parameters.\n",
        "\n",
        "Best for: Sparse data (like text data).\n",
        "\n",
        "Example: Used in natural language processing where some words appear more frequently than others.\n",
        "\n",
        "5. RMSProp\n",
        "How it works: Fixes AdaGradâ€™s issue of vanishing learning rates by using a moving average of squared gradients.\n",
        "\n",
        "Best for: Recurrent Neural Networks (RNNs) and non-stationary data.\n",
        "\n",
        "Example: Optimizing time series models like stock price predictions.\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "How it works: Combines the benefits of both Momentum and RMSProp. It adapts the learning rate for each parameter and uses moving averages of both gradients and their squares.\n",
        "\n",
        "Advantage: Fast, reliable, and widely used in deep learning.\n",
        "\n",
        "Example: Commonly used to train Convolutional Neural Networks (CNNs) for image classification."
      ],
      "metadata": {
        "id": "bMGcRC0R1imu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is sklearn.linear_model ?\n",
        "\n",
        "--> sklearn.linear_model is a module in Scikit-learn (a popular Python machine learning library) that contains classes and functions for building linear models. These are models that assume a linear relationship between the input features (independent variables) and the output (target variable). It's widely used for both regression and classification problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "8tclieUy166t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does model.fit() do? What arguments must be given?\n",
        "\n",
        "--> It takes in input data (features) and target data (labels or values).\n",
        "\n",
        "It uses this data to find the best parameters that minimize the error (loss function).\n",
        "\n",
        "After fitting, the model is ready to make predictions on new, unseen data using model.predict().\n",
        "\n",
        "Required Arguments:\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "X: The features (input data). This is usually a 2D array or DataFrame where each row is a sample and each column is a feature.\n",
        "\n",
        "y: The target (output data). This is usually a 1D array or Series with the labels or values you want to predict."
      ],
      "metadata": {
        "id": "Z4_Gl42N2RZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does model.predict() do? What arguments must be given?\n",
        "\n",
        "--> The model.predict() function in machine learning is used to generate predictions from a trained model. Once a model has been trained using model.fit(), the predict() method uses the learned parameters to predict outcomes for new, unseen input data.\n",
        "\n",
        "Required Argument:\n",
        "\n",
        "model.predict(X)\n"
      ],
      "metadata": {
        "id": "lOqB5hJn2iaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are continuous and categorical variables?\n",
        "\n",
        "--> In data analysis and machine learning, variables (also called features or attributes) are the columns of your dataset. They are usually classified into two main types: continuous and categorical.\n",
        "\n",
        "1.  Continuous Variables: Continuous variables are numeric values that can take any value within a range. They are often measurable and can have decimal points.\n",
        "\n",
        "2. Categorical Variables: Categorical variables represent distinct groups or categories. They usually take on a limited number of values and describe qualities or labels, not quantities."
      ],
      "metadata": {
        "id": "LIsMi8iA3EMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "--> Feature scaling is a technique in data preprocessing used to normalize or standardize the range of independent variables (features) in a dataset. In many machine learning algorithms, especially those that rely on distance or gradient-based optimization (like k-NN, SVM, or linear regression), having features with different scales can cause the model to behave poorly or converge slowly.\n",
        "\n",
        "How Feature Scaling Helps in Machine Learning\n",
        "Improves model performance for algorithms that are sensitive to feature magnitude.\n",
        "\n",
        "Speeds up convergence in gradient descent optimization.\n",
        "\n",
        "Ensures fairness between features when computing distances (e.g., in KNN or clustering).\n",
        "\n",
        "Prevents bias toward features with larger numerical values."
      ],
      "metadata": {
        "id": "_peENJEy3Wvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we perform scaling in Python?\n",
        "\n",
        "''' In Python, feature scaling is commonly performed using the sklearn.preprocessing module from the Scikit-learn library. It provides several tools to scale your data efficiently and consistently.'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('your_data.csv')\n",
        "\n",
        "X = df[['feature1', 'feature2', 'feature3']]  # Replace with your actual column names\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GiUgqZBs33CP",
        "outputId": "1199978f-908d-4d44-b754-620987671d63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-06f2bdfb6931>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace with your actual column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is sklearn.preprocessing?\n",
        "\n",
        "--> sklearn.preprocessing is a module in the Scikit-learn library that provides a variety of tools and utilities for preprocessing data before feeding it into machine learning models. Preprocessing is a crucial step because raw data often needs to be transformed or scaled to improve model performance and ensure the data is in a suitable format."
      ],
      "metadata": {
        "id": "BFVIoRde4pBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "--> Start with your entire dataset containing all examples and their corresponding labels (if supervised learning).\n",
        "\n",
        "Decide on the proportion of data to be used for training and testing â€” for example, 80% training and 20% testing.\n",
        "\n",
        "Randomly shuffle the dataset to ensure data points are mixed well and not ordered by any specific feature or time.\n",
        "\n",
        "Split the dataset into two parts:\n",
        "\n",
        "The training set (e.g., 80%) will be used to teach the model â€” it will learn patterns and relationships from this data.\n",
        "\n",
        "The testing set (e.g., 20%) will be kept aside and used only after training to evaluate how well the model generalizes to new data.\n",
        "\n",
        "Use the training data to fit the model, and only after training, use the test set to evaluate its performance.\n"
      ],
      "metadata": {
        "id": "16Jld--35qFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain data encoding?\n",
        "\n",
        "--> Data encoding is the process of converting data from one format into another so that machine learning algorithms can understand and work with it. This is especially important for categorical variables â€” which are often non-numeric â€” because most ML models require numerical input.\n",
        "\n"
      ],
      "metadata": {
        "id": "LiABREy26Lfh"
      }
    }
  ]
}