{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What is Simple Linear Regression?\n",
        "\n",
        "--> Simple Linear Regression is a statistical method used to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an independent variable (predictor), and the other is a dependent variable (response)."
      ],
      "metadata": {
        "id": "tHpnmSb5crDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "-->\n",
        "1. Linearity\n",
        "There is a linear relationship between the independent variable\n",
        "X and the dependent variable\n",
        "Y.\n",
        "\n",
        "  This means that the change in Y is proportional to the change in X.\n",
        "\n",
        "  Check using a scatter plot or residual plot.\n",
        "\n",
        "2. Independence of Errors\n",
        "The residuals (errors) are independent of each other.\n",
        "\n",
        "  Especially important in time series data (no autocorrelation).\n",
        "\n",
        "  Check using Durbin-Watson test.\n",
        "\n",
        "3. Homoscedasticity (Constant Variance)\n",
        "The variance of the residuals is constant across all levels of X.\n",
        "\n",
        "  No \"funnel\" or \"fan\" shape in the residual plot.\n",
        "\n",
        "  Check using residual vs fitted value plots.\n",
        "\n",
        "4. Normality of Errors\n",
        "The residuals (differences between actual and predicted Y) should be normally distributed.\n",
        "\n",
        "  Important for hypothesis testing and confidence intervals.\n",
        "\n",
        "  Check using histogram or Q-Q plot of residuals.\n",
        "\n",
        "5. No or Little Multicollinearity\n",
        "This applies more to multiple linear regression, but even in SLR, ensure the predictor is not a transformation of another hidden variable."
      ],
      "metadata": {
        "id": "hlO8QQ_lc96A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "--> the coefficient m represents the slope of the line.\n",
        "It shows how much Y changes for a one-unit increase in X.\n",
        "\n"
      ],
      "metadata": {
        "id": "eqoMrf20dj4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "--> It is the value of\n",
        "Y when X=0.\n",
        "\n",
        "Graphically, it’s the point where the line crosses the Y-axis."
      ],
      "metadata": {
        "id": "A6_Go5z0fHPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "--> In Simple Linear Regression, the slope\n",
        "m represents the rate at which the dependent variable\n",
        "Y changes with respect to the independent variable\n",
        "X. It is calculated using the least squares method, which minimizes the sum of squared differences between the actual and predicted values. The formula for the slope is:\n",
        "\n",
        "m=\n",
        "\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "ˉ\n",
        " ) /\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "Here,\n",
        "X\n",
        "i\n",
        "​and\n",
        "Y\n",
        "i\n",
        "are individual data points, and\n",
        "X\n",
        "ˉ\n",
        "  and\n",
        "Y\n",
        "ˉ\n",
        "  are the mean values of\n",
        "X and\n",
        "Y, respectively. The numerator captures the covariance between X and Y, showing how the variables change together, while the denominator measures the variance of X, indicating how spread out the X values are. Dividing these gives the average change in Y for a one-unit change in X. In simple terms, the slope tells us how much Y is expected to increase (or decrease) when X increases by one unit.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Bm3_ttwfjsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "--> The purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line (regression line) through the data points by minimizing the total error between the actual and predicted values of the dependent variable."
      ],
      "metadata": {
        "id": "_WDbZX8Igme3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "--> Interpretation of\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " :\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  represents the proportion of the variance in the dependent variable\n",
        "𝑌\n",
        "Y that is explained by the independent variable\n",
        "𝑋\n",
        "X.\n",
        "\n",
        "Its value ranges from 0 to 1:\n",
        "\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        "R\n",
        "2\n",
        " =1: Perfect fit (100% of the variance in Y is explained by X)\n",
        "\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "0\n",
        "R\n",
        "2\n",
        " =0: The model explains none of the variance in Y\n",
        "\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "0.7\n",
        "R\n",
        "2\n",
        " =0.7: 70% of the variation in Y is explained by X, the remaining 30% is due to other factors or noise"
      ],
      "metadata": {
        "id": "CIu5z5W0gy7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Multiple Linear Regression?\n",
        "\n",
        "--> Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, ..., Xₙ). It extends Simple Linear Regression, which uses only one predictor, by allowing multiple predictors to estimate the target outcome."
      ],
      "metadata": {
        "id": "l6rQOvrLhZyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "--> The main difference lies in the number of independent variables (predictors) used to predict the dependent variable. Simple Linear Regression uses one predictor to explain or predict the target variable, while Multiple Linear Regression uses two or more predictors to capture a more complex and realistic relationship."
      ],
      "metadata": {
        "id": "ali5RkTvhoJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "--> 1. Linearity\n",
        "The relationship between the dependent variable (Y) and each independent variable (X₁, X₂, ..., Xₙ) is linear.\n",
        "\n",
        "This means Y changes proportionally with changes in Xs.\n",
        "\n",
        "📈 Check: Use scatter plots or partial regression plots.\n",
        "\n",
        "🔹 2. Independence of Errors (Residuals)\n",
        "The residuals (errors) are independent — no correlation between error terms.\n",
        "\n",
        "Especially important in time series data to avoid autocorrelation.\n",
        "\n",
        "📊 Check: Durbin-Watson test.\n",
        "\n",
        "🔹 3. Homoscedasticity (Constant Variance of Errors)\n",
        "The residuals should have constant variance at all levels of the independent variables.\n",
        "\n",
        "No pattern or funnel shape should appear in residual plots.\n",
        "\n",
        "📉 Check: Plot residuals vs. fitted values.\n",
        "\n",
        "🔹 4. Normality of Errors\n",
        "The residuals should be normally distributed (especially important for confidence intervals and hypothesis testing).\n",
        "\n",
        "📊 Check: Histogram or Q-Q plot of residuals.\n",
        "\n",
        "🔹 5. No Multicollinearity\n",
        "The independent variables should not be highly correlated with each other.\n",
        "\n",
        "High multicollinearity makes it hard to isolate the individual effect of predictors.\n",
        "\n",
        "📌 Check: Variance Inflation Factor (VIF); VIF > 5 or 10 indicates a problem.\n",
        "\n",
        "🔹 6. No Measurement Error in Predictors\n",
        "The independent variables are assumed to be measured accurately."
      ],
      "metadata": {
        "id": "dxy-gK2GiAeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "--> Heteroscedasticity refers to a condition in Multiple Linear Regression where the variance of the residuals (errors) is not constant across all levels of the independent variables. In simpler terms, as the value of predictors changes, the spread (or variability) of the residuals increases or decreases.\n",
        "\n",
        "1. Inefficient Estimates\n",
        "\n",
        "The regression coefficients (slopes) remain unbiased, but they become less efficient (i.e., they have larger standard errors).\n",
        "\n",
        "2. Invalid Hypothesis Tests\n",
        "\n",
        "Standard errors become unreliable, which leads to:\n",
        "\n",
        "Incorrect t-tests\n",
        "\n",
        "Misleading p-values\n",
        "\n",
        "Inaccurate confidence intervals\n",
        "\n",
        "3. Reduced Predictive Power\n",
        "\n",
        "The model might still predict well on average, but the variability of predictions can be high, especially in regions where variance is large.\n",
        "\n"
      ],
      "metadata": {
        "id": "RKIOW5hTivl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How can you improve a Multiple Linear Regression model with high multicollinearity>\n",
        "\n",
        "--> To improve a Multiple Linear Regression model with high multicollinearity, you need to reduce the correlation between independent variables, as multicollinearity can distort the estimated coefficients, inflate standard errors, and make it difficult to determine the effect of each predictor. One common approach is to remove one or more highly correlated variables from the model, especially if they do not add significant predictive power. Alternatively, you can combine correlated variables using techniques like Principal Component Analysis (PCA), which transforms the predictors into uncorrelated components. Another method is to use regularization techniques such as Ridge Regression or Lasso Regression, which penalize large coefficients and can help manage multicollinearity while maintaining model performance. Additionally, computing the Variance Inflation Factor (VIF) for each predictor can help identify which variables are causing multicollinearity; variables with a high VIF (typically above 5 or 10) should be considered for removal or transformation. By applying these techniques, you can improve the model's interpretability and ensure more stable, reliable predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LAVQ_GW1j2Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "--> Categorical variables need to be numerically encoded before they can be used in regression models, since regression algorithms work with numerical inputs. Here are the most widely used techniques:\n",
        "\n",
        "1. One-Hot Encoding:\n",
        "Converts each category into a binary column (0 or 1).\n",
        "\n",
        "Best for nominal data (categories with no order, e.g., color: red, blue, green).\n",
        "\n",
        "2. Label Encoding:\n",
        "Assigns an integer value to each category.\n",
        "\n",
        "3. Ordinal Encoding:\n",
        "Similar to label encoding but used intentionally for ordered categories.\n",
        "\n",
        "4. Binary Encoding:\n",
        "First converts categories to binary, then splits into separate columns.\n",
        "\n",
        "More compact than one-hot encoding.\n",
        "\n",
        "5. Target Encoding (Mean Encoding):\n",
        "Replaces each category with the mean of the target variable for that category.\n",
        "\n"
      ],
      "metadata": {
        "id": "V1FVF_vSkI99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "--> Interaction terms in Multiple Linear Regression capture the combined effect of two or more independent variables on the dependent variable, beyond their individual effects. They are used when the effect of one predictor depends on the level of another.\n",
        "\n",
        "In a standard linear regression, the model assumes that each independent variable affects the dependent variable independently and additively. However, in real-world scenarios, variables often interact, meaning their joint effect is not simply the sum of individual effects."
      ],
      "metadata": {
        "id": "6C6I1fX-k7ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "--> The intercept (also called the constant term) in both Simple and Multiple Linear Regression represents the predicted value of the dependent variable (Y) when all independent variables (X’s) are equal to zero. However, its interpretation and practical meaning can differ depending on the context and number of predictors.\n",
        "\n",
        "1. In Simple Linear Regression:\n",
        "  There is only one independent variable (X), so the intercept\n",
        "c (or\n",
        "β\n",
        "0\n",
        "​\n",
        " ) represents the expected value of Y when X = 0.\n",
        "\n",
        "\n",
        "2. In Multiple Linear Regression:\n",
        "There are multiple independent variables, so the intercept represents the predicted value of Y when all X₁, X₂, ..., Xₙ = 0."
      ],
      "metadata": {
        "id": "WSJRoNMqlbMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "--> In regression analysis, the slope (also known as the regression coefficient) represents the rate of change of the dependent variable (Y) with respect to a one-unit change in an independent variable (X), while keeping all other variables constant (in Multiple Linear Regression).\n",
        "\n",
        "In regression analysis, the slope (also known as the regression coefficient) represents the rate of change of the dependent variable (Y) with respect to a one-unit change in an independent variable (X), while keeping all other variables constant (in Multiple Linear Regression).\n",
        "\n"
      ],
      "metadata": {
        "id": "aJWT2U45ueBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "--> The intercept in a regression model represents the expected value of the dependent variable (Y) when all independent variables (X's) are equal to zero. While the intercept does not explain how the predictors affect Y directly (that’s the role of the slopes), it provides important context and serves as the baseline from which changes in the predictors begin to affect the outcome.\n",
        "\n",
        "Here, the intercept\n",
        "𝑐\n",
        "c tells us the starting value of Y when\n",
        "X=0. For example, if you are modeling salary based on experience, and the intercept is 30,000, it implies someone with zero years of experience is predicted to earn ₹30,000.\n",
        "\n",
        "This helps anchor the line and gives a meaningful reference point for interpreting the slope."
      ],
      "metadata": {
        "id": "Z3YPMjHd03VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "--> While R² (coefficient of determination) is a popular metric to evaluate how well a regression model explains the variation in the dependent variable, relying on it alone can be misleading. It has several limitations that can mask poor model quality or overstate its usefulness.\n",
        "\n",
        "1. Does Not Indicate Causality\n",
        "R² only shows correlation, not causation. A high R² value does not mean that the independent variables cause changes in the dependent variable.\n",
        "\n",
        "2. Ignores Model Complexity\n",
        "R² always increases (or stays the same) when more predictors are added, even if those predictors do not improve the model significantly. This can lead to overfitting — where the model fits the training data well but performs poorly on unseen data.\n",
        "\n",
        "3. Does Not Reflect Predictive Accuracy\n",
        "A high R² value does not guarantee accurate predictions, especially when the model violates assumptions (e.g., linearity, homoscedasticity). You can have a high R² and still make poor predictions.\n",
        "\n",
        "4. Sensitive to Outliers\n",
        "R² can be inflated by outliers or extreme values, giving a false impression of model performance. Outliers can skew the regression line to appear better fitting than it is.\n",
        "\n",
        "5. Not Suitable for Non-Linear Models\n",
        "R² is most meaningful for linear relationships. For non-linear models, it can give misleading results, even when the model performs well.\n",
        "\n",
        "6. Does Not Capture Bias or Error Distribution\n",
        "R² doesn't tell you whether the residuals are biased, non-normal, or if the model violates assumptions. It says nothing about how errors are distributed."
      ],
      "metadata": {
        "id": "Kdtv6hJU1aSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "--> A large standard error for a regression coefficient indicates a high level of uncertainty about the estimated value of that coefficient. In regression analysis, the standard error measures how precisely a coefficient is estimated from the sample data — smaller values indicate more confidence, while larger values suggest less reliable estimates.\n",
        "\n",
        "1. Interpretation:\n",
        "Unstable Coefficient Estimate\n",
        "A large standard error suggests that if you were to repeat the analysis with different samples, the value of the coefficient might vary significantly. This undermines the trustworthiness of that predictor.\n",
        "\n",
        "2. Low Statistical Significance\n",
        "It often leads to a high p-value, implying that the coefficient may not be significantly different from zero — meaning the variable might not be contributing meaningfully to the model.\n",
        "\n",
        "3. Multicollinearity Warning\n",
        "A common cause of large standard errors is multicollinearity — when two or more independent variables are highly correlated. This makes it difficult for the model to isolate the individual effect of each variable.\n",
        "\n",
        "4. Weak Relationship\n",
        "It may suggest that the predictor does not have a strong linear relationship with the dependent variable, or the relationship is noisy and inconsistent."
      ],
      "metadata": {
        "id": "UaB2nVDz57v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "--> Heteroscedasticity refers to the condition where the variance of residuals (errors) is not constant across all levels of the independent variable(s). In simpler terms, the spread of the prediction errors changes depending on the value of X — violating a key assumption of linear regression.\n",
        "\n",
        "A residual plot displays the residuals (errors) on the y-axis and the predicted values (or one of the independent variables) on the x-axis.\n",
        "\n",
        "Signs of Heteroscedasticity:\n",
        "\n",
        "The residuals form a funnel shape — they spread out or narrow as X increases.\n",
        "\n",
        "You see patterns, like a fan, cone, or curve, rather than a random cloud of points.\n",
        "\n",
        "Variance appears to increase or decrease systematically with the predictor."
      ],
      "metadata": {
        "id": "fl2uoQGx6Xv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "--> When a Multiple Linear Regression model shows a high R² but a low Adjusted R², it suggests that some of the independent variables are not actually contributing to explaining the variability in the dependent variable — and may even be hurting the model.\n",
        "\n",
        "R² (Coefficient of Determination):\n",
        "Measures how well the model explains the variance in the dependent variable.\n",
        "Always increases (or stays the same) when more variables are added — even if those variables are irrelevant.\n",
        "\n",
        "Adjusted R²:\n",
        "Adjusts R² for the number of predictors in the model. It penalizes the inclusion of unnecessary predictors.\n",
        "It can decrease if a newly added variable doesn't improve the model meaningfully.\n",
        "\n"
      ],
      "metadata": {
        "id": "kTB-PwBs69xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "--> Scaling variables in Multiple Linear Regression is important primarily for interpretability, numerical stability, and regularization—especially when the independent variables (features) are on very different scales.\n",
        "\n",
        "1. Improves Interpretability of Coefficients\n",
        "When variables are on vastly different scales (e.g., age in years vs. income in thousands), their regression coefficients are also on different scales, making it difficult to:\n",
        "\n",
        "Compare their influence on the target variable.\n",
        "\n",
        "Interpret the size of the effect properly.\n",
        "\n",
        "2. Helps with Numerical Stability\n",
        "Regression involves matrix operations. When variables have different magnitudes, these computations can lead to numerical instability or poorly conditioned matrices, which may:\n",
        "\n",
        "Result in inaccurate coefficient estimates.\n",
        "\n",
        "Cause convergence issues in optimization.\n",
        "\n",
        "3. Essential for Regularization (Ridge/Lasso)\n",
        "If you're using regularized versions of regression (like Ridge or Lasso), scaling is crucial because:\n",
        "\n",
        "These models penalize large coefficients.\n",
        "\n",
        "A variable with a large scale can dominate the penalty and distort the model if not scaled.\n",
        "\n",
        "4. Reduces Bias in Feature Selection:\n",
        "Without scaling, variables with larger scales may appear more important just because of their magnitude, not their actual predictive power.\n",
        "\n"
      ],
      "metadata": {
        "id": "fQK6mgBV7Z0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is polynomial regression?\n",
        "\n",
        "--> Polynomial Regression is a type of regression analysis that models the relationship between the independent variable (X) and the dependent variable (Y) as an nth-degree polynomial. It is an extension of Simple Linear Regression that allows us to capture non-linear relationships by adding higher-order terms (like\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ,…).\n",
        "\n"
      ],
      "metadata": {
        "id": "_KYf8gYa7847"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does polynomial regression differ from linear regression?\n",
        "\n",
        "--> Polynomial regression differs from linear regression primarily in the type of relationship it models between the independent and dependent variables. While linear regression assumes a straight-line relationship and fits a linear equation of the form\n",
        "Y=β\n",
        "0\n",
        "​\n",
        "+β\n",
        "1\n",
        "​\n",
        " X, polynomial regression allows for a curved relationship by including higher-order powers of the independent variable, such as\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " , and so on. This makes polynomial regression more flexible and capable of fitting data where trends are non-linear. Despite its flexibility, polynomial regression can easily overfit the data, especially when the degree of the polynomial is too high, capturing noise rather than meaningful patterns. Unlike linear regression, which is generally easier to interpret and less prone to overfitting, polynomial regression requires careful selection of the polynomial degree and sometimes scaling of variables to maintain stability. Both models use linear combinations of coefficients, but polynomial regression transforms the input features to capture more complex relationships in the data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Jm-P4VQ8Na6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When is polynomial regression used?\n",
        "\n",
        "--> Curved Data Patterns\n",
        "When a scatter plot shows a non-linear trend (e.g., U-shaped or S-shaped), polynomial regression can model that curvature effectively, unlike simple linear regression which only fits a straight line.\n",
        "\n",
        "Modeling Natural Phenomena\n",
        "It’s used in physics, biology, and environmental science where real-world relationships often follow non-linear patterns — such as population growth, chemical reaction rates, or projectile motion.\n",
        "\n",
        "Economics and Finance\n",
        "Useful in modeling cost curves, profit maximization, or market trends that show diminishing returns or inverted U-shaped behavior.\n",
        "\n",
        "Engineering Applications\n",
        "For fitting calibration curves, sensor data, or other systems where the relationship between variables is not linear but still smooth and continuous.\n",
        "\n",
        "Predictive Modeling\n",
        "When simple models underfit the data, polynomial regression can improve prediction accuracy without switching to more complex algorithms like decision trees or neural networks."
      ],
      "metadata": {
        "id": "TUtefxOL9Kf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What is the general equation for polynomial regression?\n",
        "\n",
        "--> he general equation for polynomial regression of degree n is:\n",
        "\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ε"
      ],
      "metadata": {
        "id": "g4R7uyf8_c2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "--> Yes, polynomial regression can be applied to multiple variables — this is known as multivariate polynomial regression. In this case, the model includes not only the linear terms of each independent variable but also higher-order terms (like squares and cubes) and even interaction terms between variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "buP7t_U-_8-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the limitations of polynomial regression?\n",
        "\n",
        "-->  1. Overfitting\n",
        "As the degree of the polynomial increases, the model becomes more complex and flexible. This can lead to overfitting, where the model fits the training data perfectly — including noise and outliers — but performs poorly on unseen data.\n",
        "\n",
        "2. High Variance\n",
        "Higher-degree polynomials can result in large swings or oscillations in the prediction curve, especially near the boundaries of the data. This makes the model unstable and sensitive to slight changes in data.\n",
        "\n",
        "3. Interpretability\n",
        "With increasing degree, the model becomes harder to interpret. While a linear or quadratic model might be understandable, a 5th or 6th-degree polynomial with interaction terms is not intuitive and lacks practical meaning.\n",
        "\n",
        "4. Computational Complexity\n",
        "As the number of predictors or polynomial degree increases, the number of terms grows exponentially, leading to:\n",
        "\n",
        "Longer training time\n",
        "\n",
        "Higher memory use\n",
        "\n",
        "Difficulty in managing multicollinearity\n",
        "\n",
        "5. Poor Extrapolation\n",
        "Polynomial regression is very unreliable for predictions outside the range of the training data. The curve can shoot up or down unpredictably at the ends, making extrapolated predictions highly inaccurate.\n",
        "\n",
        "6. Risk of Multicollinearity\n",
        "Adding powers of variables (e.g.,\n",
        "𝑋\n",
        "X,\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        " ,\n",
        "𝑋\n",
        "3\n",
        " ) often leads to strong correlations between features, which can inflate variance of coefficients and reduce model stability.\n",
        "\n"
      ],
      "metadata": {
        "id": "OR32G7sRANUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "-->\n",
        " 1. Visual Inspection (Residual and Fit Plots)\n",
        "Plot the fitted curve against the data.\n",
        "\n",
        "Look at residual plots — residuals should be randomly scattered without a pattern.\n",
        "\n",
        "A curve that’s too wavy suggests overfitting, while a poor fit suggests underfitting.\n",
        "\n",
        "2. Mean Squared Error (MSE) / Root Mean Squared Error (RMSE)\n",
        "Compute MSE or RMSE on the training and validation datasets.\n",
        "\n",
        "A large gap between training and validation errors implies overfitting.\n",
        "\n",
        "3. Cross-Validation (e.g., K-Fold)\n",
        "Perform K-Fold Cross-Validation to test the model on multiple subsets of the data.\n",
        "\n",
        "Helps evaluate generalization performance and choose a degree that works well across folds.\n",
        "\n",
        "4. Adjusted R²\n",
        "Adjusted R² accounts for the number of predictors and penalizes complexity.\n",
        "\n",
        "If R² increases but adjusted R² decreases, it means the added complexity is not worth it.\n",
        "\n",
        "5. AIC (Akaike Information Criterion) / BIC (Bayesian Information Criterion)\n",
        "Both penalize model complexity.\n",
        "\n",
        "Lower values indicate better models.\n",
        "\n",
        "Helps in selecting a model that balances fit and simplicity.\n",
        "\n",
        "6. Learning Curves\n",
        "Plot training and validation errors vs. model complexity (degree).\n",
        "\n",
        "Helps visualize at which point the model starts to overfit.\n",
        "\n"
      ],
      "metadata": {
        "id": "aqd4tubIAkXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is visualization important in polynomial regression?\n",
        "\n",
        "--> Visualization plays a crucial role in polynomial regression because it helps you understand the relationship between variables, detect model issues, and choose the right model complexity. Unlike simple linear regression where the relationship is straight, polynomial regression involves curved trends, making visuals even more essential."
      ],
      "metadata": {
        "id": "T46qu6GEBDRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How is polynomial regression implemented in Python?\n",
        "\n",
        "--> Polynomial regression in Python is commonly implemented using Scikit-learn, a powerful ML library. It involves transforming input features into polynomial features and then applying linear regression on them.\n",
        "Polynomial regression in Python is implemented by first using PolynomialFeatures to transform the input, then applying LinearRegression. It’s simple and effective for modeling curved relationships, with visualization helping to assess the fit quality."
      ],
      "metadata": {
        "id": "ttcpjLNbBLA7"
      }
    }
  ]
}